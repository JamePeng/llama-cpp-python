name: Tests

on:
  pull_request:
    branches: ["main"]
  push:
    branches: ["main"]

# Auto-cancel stale runs on the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  REPO_ID: Qwen/Qwen2.5-0.5B-Instruct-GGUF
  MODEL_FILE: qwen2.5-0.5b-instruct-q4_k_m.gguf
  HF_HOME: ${{ github.workspace }}/hf_cache

jobs:
  # Combined job for Linux, Windows, and macOS (non-Metal)
  build-and-test:
    runs-on: ${{ matrix.os }}
    strategy:
      # Don't cancel other jobs in the matrix if one fails
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ["3.9", "3.13", "3.14"]
        include:
          # macOS Non-Metal
          - os: macos-14
            python-version: "3.9"
            cmake_args: "-DLLAMA_METAL=off"
            metal_status: "(No Metal)"
          - os: macos-14
            python-version: "3.13"
            cmake_args: "-DLLAMA_METAL=off"
            metal_status: "(No Metal)"
          - os: macos-14
            python-version: "3.14"
            cmake_args: "-DLLAMA_METAL=off"
            metal_status: "(No Metal)"

          # macOS Metal
          - os: macos-14
            python-version: "3.9"
            cmake_args: "-DLLAMA_METAL=on -DGGML_METAL_USE_BF16=on -DGGML_METAL_EMBED_LIBRARY=on"
            metal_status: "(Metal)"
          - os: macos-14
            python-version: "3.13"
            cmake_args: "-DLLAMA_METAL=on -DGGML_METAL_USE_BF16=on -DGGML_METAL_EMBED_LIBRARY=on"
            metal_status: "(Metal)"
          - os: macos-14
            python-version: "3.14"
            cmake_args: "-DLLAMA_METAL=on -DGGML_METAL_USE_BF16=on -DGGML_METAL_EMBED_LIBRARY=on"
            metal_status: "(Metal)"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          submodules: "recursive"

      - name: Install the latest version of uv and set the python version
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache HuggingFace model
        id: model-cache
        uses: actions/cache@v5
        with:
          path: ${{ env.HF_HOME }}
          key: ${{ runner.os }}-model-${{ env.REPO_ID }}-${{ env.MODEL_FILE }}

      - name: Download model if not cached
        # Only run this step if the cache was not found
        if: steps.model-cache.outputs.cache-hit != 'true'
        run: |
          uv pip install --system huggingface-hub
          hf download ${{ env.REPO_ID }} ${{ env.MODEL_FILE }}
        shell: bash

      - name: Install dependencies and Build
        env:
          CMAKE_ARGS: ${{ matrix.cmake_args }}
        run: |
          echo "Building with CMAKE_ARGS: $CMAKE_ARGS"
          uv pip install --system -e .[all] --verbose
        shell: bash

      - name: System Info and llama-cpp-python version
        run: |
          python -c "import platform; print('Machine:', platform.machine(), 'Arch:', platform.architecture())"
          if [[ "${{ runner.os }}" == "macOS" ]]; then
             sysctl -n machdep.cpu.brand_string
          fi
          python -c "import llama_cpp; print('llama_cpp_python:', llama_cpp.__version__)"
        shell: bash

      - name: Test with pytest by python ${{ matrix.python-version }}
        run: python -m pytest
        shell: bash